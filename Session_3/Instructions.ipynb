{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c65117c1",
   "metadata": {},
   "source": [
    "## Coding a Conformational Transition Workflow with Crossflow\n",
    "\n",
    "In this session we will step through the process of coding an enhanced sampling workflow to map the conformational transition of adenylate kinase between the \"closed\" and \"open\" conformations.\n",
    "\n",
    "### Step 1: Orientation\n",
    "\n",
    "In your Jupyterlab window, navigate to the *Session_3* folder. There you will find six files:\n",
    "* *1ake_em.gro* and *1ake.top*: Gromacs coordinate and topology files respectively for the closed conformational state.\n",
    "* *4ake_em.gro* and *4ake.top*: the same for the open state. \n",
    "* *nvt.mdp*: an input file that defines parameters for a short MD simulation.\n",
    "* *slurmscript.sh*: a Slurm job submission file.\n",
    "\n",
    "### Step 2: Run a test MD job\n",
    "\n",
    "Before we start coding up a workflow that will end up running large numbers of MD simulations, let's make sure we can run one the \"normal\" way.\n",
    "\n",
    "From your Jupyterlab window start a terminal session, and then open the file *slurmscript.sh* in an editor of your choice. If you have used Gromacs on Archer2 before, it should look fairly familiar. Key points to note are:\n",
    "\n",
    "* For this workshop jobs will use the \"e280-workshop_1018917\" partition (`-p` option).\n",
    "* Jobs will be charged to the \"e280-workshop\" account (`-a` option).\n",
    "* Jobs will use the \"reservation\" quality of service parameter (`-q` option).\n",
    "\n",
    "As usual, runing a Gromacs MD job is a 2-part process; first the coordinate, parameter and job definition files are processed by *grompp* to give a \"portable run file\" (in this case, *1ake_nvt.tpr*), then the *-deffnm* argument to the following *mdrun* command defines that all input and output files for the MD run itself will have a base filename of *1ake_nvt*. Finally this script also includes a post-processing step: the Gromacs *trjconv* utility is used to post-process the trajectory file to remove artifacts due to the use of periodic boundary conditions. Notice on this line how we get round the issue that *trjconv* is usually run interactively, and asks for user input (which group to process).\n",
    "\n",
    "Once you have satified yourself that the slurmscript is correct, submit the job in the usual way:\n",
    "```bash\n",
    "sbatch slurmscript.sh\n",
    "```\n",
    "Use *squeue -u \\<username\\>* to follow the job; because we have a reservation on Archer2 for this workshop it should start almost straight away, and finish in less than a minute.\n",
    "\n",
    "You will see the job generates a set of output files, all with names that begin *1ake_nvt*, with the extension defining the file type according to the usual Gromacs conventions.\n",
    "\n",
    "### Step 3: Build a Crossflow Cluster to Run the Same Job\n",
    "\n",
    "Now let's see how we can run the same MD simulation workflow, but via Crossflow.\n",
    "\n",
    "Start a new Jupyter notebook, and copy the following into the first cell:\n",
    "\n",
    "```python\n",
    "from crossflow.tasks import SubprocessTask\n",
    "from crossflow.clients import Client\n",
    "from dask_jobqueue import SLURMCluster\n",
    "```\n",
    "\n",
    "The first two lines should be familiar from the earler session. In that we connected a crssflow *client* to a *cluster* that was actually another process running on the same machine, using the *LocalCluster* class. In this case in line 3 we import the *SLURMCluster* class from the *dask_jobqueue* package, which will allow us to create clusters of (maybe many) workers that are individual Archer2 nodes.\n",
    "\n",
    "Run this cell (there should be no output), then copy the following into the cell below:\n",
    "\n",
    "```python\n",
    "cluster = SLURMCluster(cores=1, \n",
    "                       job_cpu=1,\n",
    "                       processes=1,\n",
    "                       memory='256GB',\n",
    "                       queue='e280-workshop_1018917',\n",
    "                       job_directives_skip=['--mem', '-n '],\n",
    "                       interface='hsn0',\n",
    "                       job_extra_directives=['--nodes=1', \n",
    "                           '--qos=\"reservation\"', \n",
    "                           '--tasks-per-node=128'],\n",
    "                       python='python',\n",
    "                       account='e280-workshop',\n",
    "                       walltime=\"06:00:00\",\n",
    "                       shebang=\"#!/bin/bash --login\",\n",
    "                       local_directory='$PWD',\n",
    "                       job_script_prologue=['module load gromacs',\n",
    "                                  'export OMP_NUM_THREADS=1',\n",
    "                                  'source /work/e280/e280/<username>/myvenv/bin/activate'])\n",
    "```\n",
    "There is a lot to look at here; most of it is \"boilerplate\" and you should take a look at the *dask_jobqueue* [documentation](here) if you want a full explanation, but for now we just pull out a few key features:\n",
    "\n",
    "* the first three arguments (`cores`, `job_cpu`, and `processes`) ensure that each worker starts on, and has full access to, a full node on Archer2.\n",
    "* The `interface=\"hsn0\"` argument ensures that communication between workers in the cluster will use the system's fast interconnect.\n",
    "* The list passed to the `job_script_prologue` argument is where commands that set up the computing environment are added. **Note you will need to edit this to insert your correct username**.\n",
    "\n",
    "Once edited, run the cell. It should run fast, as all that happens here is that the configuration of the cluster is defined - no computing resources are generated at this point.\n",
    "\n",
    "That happens next: copy the next two lines into a fresh cell and run it:\n",
    "\n",
    "```python\n",
    "client = Client(cluster)\n",
    "cluster.scale(1) # start a single worker\n",
    "```\n",
    "After a short time you should see a logfile for a slurm job appear in the file browser pane on the left of your JupyterLab window - the worker has been launched and is now ready to receive tasks.\n",
    "\n",
    "### Step 4: Build and Execute the Crossflow Tasks to Run the Same Job\n",
    "\n",
    "From the file *slurmscript.sh* we can work out the template strings required to create Crossflow *Task* that would run an equivalent job:\n",
    "\n",
    "```python\n",
    "grompp = SubprocessTask('gmx grompp -f x.mdp -c x.gro -p x.top -o system.tpr -maxwarn 1')\n",
    "grompp.set_inputs(['x.mdp', 'x.gro', 'x.top'])\n",
    "grompp.set_outputs(['system.tpr'])\n",
    "\n",
    "mdrun = SubprocessTask('srun --distribution=block:block --hint=nomultithread gmx_mpi mdrun  -deffnm system')\n",
    "mdrun.set_inputs(['system.tpr'])\n",
    "mdrun.set_outputs(['system.log', 'system.trr'])\n",
    "\n",
    "makewhole = SubprocessTask('echo 0 | gmx trjconv -f broken.trr -s system.tpr -o whole.trr -pbc whole')\n",
    "makewhole.set_inputs(['broken.trr', 'system.tpr'])\n",
    "makewhole.set_outputs(['whole.trr'])\n",
    "```\n",
    "\n",
    "This creates a *Task* called *grompp* that takes three input arguments - an mdp file, a coordinates file, and a topology file - and returns one output: a tpr file, a second *Task* called *mdrun* that takes a tpr file as input and returns two output files: a log file and a trajectory file, and a final *Task* called *makewhole* that takes a trajectory file and tpr file as input, and returns a new trajectory file.\n",
    "\n",
    "Notice that the *mdrun* task construction ignores many of the output files that the MD run will generate (e.g. the checkpoint file and energy info file). This is the normal way of working with *Crossflow* - whatever the underlying command line tool might do, the *Task* is specified in whatever way is needed to return just the information of interest, all other outputs are discarded.\n",
    "\n",
    "Copy these lines into a fresh cell and execute it (there should be no output).\n",
    "\n",
    "Now we are ready to run the job. This runs MD from the first \"end\" of the pathway we want to explore, we will label this \"A\". Here's the code, put it into a fresh cell and run it:\n",
    "\n",
    "```python\n",
    "tprA = client.submit(grompp, 'nvt.mdp', '1ake_em.gro', '1ake.top')\n",
    "logA, trajA = client.submit(mdrun, tprA)\n",
    "wholeA = client.submit(makewhole, trajA, tprA)\n",
    "print(logA)\n",
    "print(trajA)\n",
    "print(wholeA)\n",
    "```\n",
    "\n",
    "When you run this cell you will see that *logA*, *trajA* and *wholeA* are *futures* with a status of *pending* - the job has not finished yet.\n",
    "\n",
    "Copy the three *print()* statements above into a fresh cell, wait a few seconds and then run it. Has the status of the futures changed? If not, wait a bit and run again, until they show as \"finished\".\n",
    "\n",
    "If you look in the file browser pane you will see no new files seem to have been generated. This is because *crossflow* tasks are run in temporary \"sandbox\" folders, and information is only returned as *filehandles*. If you want to generate \"real\" output files, you need to make use of each filehandle's *.save()* method:\n",
    "\n",
    "```python\n",
    "logA.result().save('1ake_test.log')\n",
    "wholeA.result().save('1ake_test.trr')\n",
    "```\n",
    "\n",
    "Copy this into a new cell and run it, and convince yourself that the file *1ake_test.log* matches *1ake_nvt.log* you generated in step 2.\n",
    "\n",
    "### Interlude: Explore scaling and performance\n",
    "\n",
    "To recap, you run *Task*s on the *cluster* via the *client*'s *.submit()* method, or * *map()* method if you have many jobs you can (potentially) run in parallel. You also know that the results from *submit()* and *map()* calls are returned as *Future*s for *FileHandle*s. Input arguments in these calls can also be *Future*s or *FileHandle*s, but for convenience they can also just be the names of files (i.e., *Path*s). *Crossflow* jobs generally run more efficiently if the inputs are not *Path*s, so to explore performance we will preprocess the starting coordinate files, topology files and mdp file into *FileHandles* in advance. To do this we create an instance of a *Crossflow* *FileHandler*; here is the code:\n",
    "```python\n",
    "from crossflow.filehandling import FileHandler\n",
    "fh = FileHandler()\n",
    "startcrdsA = fh.load('1ake_em.gro')\n",
    "topA = fh.load('1ake.top')\n",
    "mdp = fh.load('nvt.mdp')\n",
    "```\n",
    "\n",
    "Copy this code into a new cell and run it - there should be no output.\n",
    "\n",
    "Now we will run the workflow again, but this time add some instrumentation so we can track the progress of the different tasks. We will also not just process the starting coordinates once, but run the workflow ten times, i.e. running ten independent short MD simulations starting from the same structure - for this we will use the *map()* method. The code is here:\n",
    "\n",
    "```python\n",
    "from distributed import wait, as_completed\n",
    "n_reps = 10\n",
    "startcrdsAs = [startcrdsA] * n_reps # replicate the starting structure\n",
    "tprAs = client.map(grompp, mdp, startcrdsAs, topA)\n",
    "logAs, trajAs = client.map(mdrun, tprAs)\n",
    "wholeAs = client.map(makewhole, trajAs)\n",
    "for future in as_completed(tprAs + trajAs + wholeAs):\n",
    "    if future in tprAs:\n",
    "        print(f'grompp job {tprAs.index(future)} completed')\n",
    "    elif future in trajAs:\n",
    "        print(f'mdrun job {trajAs.index(future)} completed')\n",
    "    else:\n",
    "        print(f'make_whole job {wholeAs.index(future)} completed')\n",
    "```\n",
    "Copy this code into a fresh cell and run it. Notice how the different tasks end up being scheduled on the cluster - which currently only has one worker (i.e., one Archer2 node).\n",
    "\n",
    "Once it has finished, copy this code into a fresh cell and run it to scale up your cluster to 5 workers:\n",
    "```python\n",
    "cluster.scale(5)\n",
    "```\n",
    "if you look at the file browser pane on the left, you will soon see new slurm log files appearing, as the jobs to run the extra workers are launched.\n",
    "\n",
    "Now go back to the previous cell and re-run it, to see how the extra workers mean the tasks can get processed in parallel.\n",
    "\n",
    "Each of you has up to ten workers available to you - so spend some time exploring how scaling the cluster speeds up the workflow.\n",
    "\n",
    "### Step 5: Run an MD Step from the 4AKE Endpoint\n",
    "\n",
    "Back to our path sampling workflow. Using the same approach, we can generate our first MD trajectory that begins from the other end (\"B\") of the pathway we want to uncover:\n",
    "\n",
    "```python\n",
    "startcrdsB = fh.load('4ake_em.gro')\n",
    "topB = fh.load('4ake.top')\n",
    "\n",
    "cluster.scale(4) # Reset the size of the cluster to a moderate value\n",
    "tprB = client.submit(grompp, mdp, startcrdsB, topB)\n",
    "logB, trajB = client.submit(mdrun, tprB)\n",
    "wholeB = client.submit(makewhole, trajB, tprB)\n",
    "```\n",
    "Copy this into a new cell and run it, but rather than wait for the job to complete, we can begin to consider the next step in the workflow.\n",
    "\n",
    "### Step 6: Coding up the RMSD Calculation Step\n",
    "\n",
    "If you look back at the workflow diagram presented in the session introduction, you will see that the next step will be to load the snapshots from both of the newly-run MD simulations into trajectory databases, one for each end of the path, and then calculate the full matrix of RMSD values between the two.\n",
    "\n",
    "There is no Gromacs command that will calculate the RMSD of every snapshot in one trajectory file from every snapshot in another, so we are going to turn our attention to Python packages to do this - *MDTraj* to import the trajectory data, and *MDPlus* to perform the RMSD calculation using a \"trick\" that makes this much faster.\n",
    "\n",
    "The discussion here will not go into the details of how *MDTraj* and *MDPlus* work, please refer to the online documentation for that if you are unfamilar with these packages.\n",
    "\n",
    "We begin by creating two *MDTraj* *Trajectory* objects, one for snapshots generated from each \"end\" of our conformational transition (we are calling the objects \"ensembles\" because ultimately they will hold data from many different MD simulations):\n",
    "\n",
    "```python\n",
    "import mdtraj as mdt\n",
    "\n",
    "ensembleA = mdt.load(wholeA.result(), top=startcrdsA)\n",
    "ensembleB = mdt.load(wholeB.result(), top=startcrdsB)\n",
    "```\n",
    "\n",
    "Notice here how we pass *filehandles* for the trajectory and topology/coordinate files to *MDTraj*'s *load()* command - this works because *filehandles* are \"pathlike\".\n",
    "\n",
    "Copy and paste this code into a fresh cell in your notebook and run it - there should be no output, but it may take a little time to run - this code is actually running in your notebook, not on the Crossflow cluster.\n",
    "\n",
    "\n",
    "Doing an all-against-all RMSD calculation is computationally expensive, but there is a \"trick\" you can use to do it much faster - if slightly approximately - using Principal Component Analysis. The method involves performing PCA analysis to tranform both trajectories into the same PC space, then calculating the Cartesian distance between points in this space. This number, divided by the square root of the number of atoms, in general is a very close approximation to the RMSD. The code is here:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from mdplus.pca import PCA\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def rmsd2(ensembleA, ensembleB, sel):\n",
    "    \"\"\"\n",
    "    Calculate approximate 2D RMSD matrix via PCA\n",
    "\n",
    "    Args:\n",
    "\n",
    "        ensembleA (mdtraj trajectory): ensemble A of nA frames\n",
    "        ensembleB (mdtraj trajectory): ensemble B of nB frames\n",
    "        sel (string): mdtraj selection specifier for RMSD calculation\n",
    "\n",
    "    Returns:\n",
    "\n",
    "        rmsd2d (numpy array[nA, nB]): RMSD matrix\n",
    "    \"\"\"\n",
    "    idxA = ensembleA.topology.select(sel)\n",
    "    idxB = ensembleB.topology.select(sel)\n",
    "    x = np.concatenate([ensembleA.xyz[:,idxA], ensembleB.xyz[:, idxB]])\n",
    "    p = PCA()\n",
    "    scores = p.fit_transform(x)\n",
    "    d = cdist(scores[:len(ensembleA)], scores[len(ensembleA):])\n",
    "    return d / np.sqrt(len(idxA))\n",
    "```\n",
    "\n",
    "Copy and paste this code into a fresh cell in your notebook and run it - there should be no output.\n",
    "\n",
    "### Step 7: Coding up and Running the Pair Selection Step\n",
    "\n",
    "Again, looking back at the workflow diagram you will see the next step involves extracting the *N* shortest distances (RMSDs) between structures in the two ensembles. There are many possible approaches to this, the one we use here puts the shortests pairs into a dictionary structure:\n",
    "\n",
    "```python\n",
    "def get_pair_distances(ensembleA, ensembleB, selection='name CA', max_pairs=10):\n",
    "    '''\n",
    "    Calculate the RMSD distances between all snapshots in each of two ensembles and return the closest\n",
    "\n",
    "    Args:\n",
    "       ensembleA (mdtraj trajectory): first ensemble\n",
    "       ensembleB (mdtraj trajectory): second ensemble\n",
    "       selection (string): mdtraj selection for RMSD calculation\n",
    "       max_pairs (int): number of closest pairs to return\n",
    "\n",
    "    Returns:\n",
    "       pairlist: sorted dictionary with tuple of snaphot indices as keys, RMSDs as values\n",
    "    '''\n",
    "    pairdist = {}\n",
    "    d = rmsd2(ensembleA, ensembleB, selection)\n",
    "    for i in range(ensembleA.n_frames):\n",
    "        for j in range(ensembleB.n_frames):\n",
    "            key = (i, j)\n",
    "            pairdist[key] = d[i, j]\n",
    "        \n",
    "    # sort by increasing RMSD:\n",
    "    pairdist = {k:v for k, v in sorted(pairdist.items(), key=lambda i: i[1])[:max_pairs]}\n",
    "    return pairdist\n",
    "\n",
    "# Now run it:\n",
    "closest_pairs = get_pair_distances(ensembleA, ensembleB, max_pairs=5)\n",
    "print(closest_pairs)\n",
    "```\n",
    "Copy and paste this code into a fresh cell in your notebook and run it - you should end up seeing the five shortest distances (RMSDs) between structures in ensembleA and structures in ensembleB (the RMSD calculation being done just over Calpha atoms).\n",
    "\n",
    "### Step 8: The Complete Workflow\n",
    "\n",
    "We are ready to run the complete workflow now. Each iteration will involve:\n",
    "\n",
    "1. Running an MD simulation on structures from each end of the *N* shortest inter-ensemble distances.\n",
    "2. Processing the trajectories to make them \"whole\"\n",
    "3. Adding the new structures to the growing ensembles \"A\" and \"B\".\n",
    "4. Updating the list of *N* shortest inter-ensemble distances\n",
    "5. Stopping if the the shortest distance is less than some threshold value, or going back to step 1.\n",
    "\n",
    "Here is the code:\n",
    "\n",
    "```python\n",
    "max_cycles = 10 # Maximum number of workflow iterations\n",
    "min_rmsd = 0.2 # Target minimum RMSD between structures from each ensemble\n",
    "max_pairs = 5 # Number of shortest inter-ensemble pairs to take forward to next iteration\n",
    "cluster.scale(max_pairs) # Scale the SLURMCluster up to max_pairs workers\n",
    "\n",
    "for icycle in range(max_cycles):\n",
    "    print(f'Starting cycle {icycle}...')\n",
    "    shortestA = [k[0] for k in list(closest_pairs.keys())] # indices of chosen structures from A\n",
    "    shortestB = [k[1] for k in list(closest_pairs.keys())] # ditto for B\n",
    "    tprAs = client.map(grompp, mdp, [ensembleA[i] for i in shortestA], topA) # run grompp jobs in parallel\n",
    "    tprBs = client.map(grompp, mdp, [ensembleB[i] for i in shortestB], topB) # ditto\n",
    "    logAs, trajAs = client.map(mdrun, tprAs) # run MD jobs in parallel\n",
    "    logBs, trajBs = client.map(mdrun, tprBs) # ditto\n",
    "    wholeAs = client.map(makewhole, trajAs, tprAs) # remove PBC artifacts\n",
    "    wholeBs = client.map(makewhole, trajBs, tprBs)\n",
    "    wait(wholeBs) # Block here until all jobs are done\n",
    "    print('MD runs finished, finding closest pairs...')\n",
    "    ensembleA += mdt.load([t.result() for t in wholeAs], top='1ake_em.gro') # Add to ensembles\n",
    "    ensembleB += mdt.load([t.result() for t in wholeBs], top='4ake_em.gro')\n",
    "    closest_pairs = get_pair_distances(ensembleA, ensembleB, max_pairs=max_pairs)\n",
    "    print(f'Cycle {icycle}: closest pairs = {closest_pairs}')\n",
    "    if list(closest_pairs.values())[0] < min_rmsd: # The two ends of the path have \"met\" - stop.\n",
    "        break\n",
    "        \n",
    "print('Path search completed')\n",
    "cluster.scale(0) # Scale the cluster back to having no workers at all.\n",
    "ensembleA.save('ensembleA.xtc') # Save the ensembles in Gromacs xtc format for later analysis.\n",
    "ensembleB.save('ensembleB.xtc')\n",
    "```\n",
    "\n",
    "Copy this into a fresh cell and run. This is going to take some time, but as each iteration finishes you should see the shortest distance between sytuctures in the two ensembles gradually reduce, until it reaches 0.2 nanometyers, when the search will stop, and the ensembles generated from each start-point will be saved to disk in Groams .xtc format. In the next session we will take a look at the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7858884e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
